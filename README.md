Multimodal Affective Feedback Synthesis for Text and Image Data
================================================

Implementation for the paper (submitted to IEEE Signal Processing Letters Journal). <br>
**[Multimodal Affective Feedback Synthesis for Text and Image Data][1]**<br>
[Puneet Kumar](https://puneet-kr.github.io/), [Gaurav Bhatt](http://deeplearn-ai.com/), Omkar Ingle, Daksh Goyal and [Balasubramanian Raman](http://faculty.iitr.ac.in/~balarfma/)  

## Code Files
The code files are currently private as the corresponding research paper in ICASSP 2022 is under review. They will be made publically available soon after the paper is published/accepted for publication.

Dataset Access
--------------
Access to the ‘IIT Roorkee Multimodal Feedback (IIT-R MMFeed) dataset’ can be obtained by through [`Access Form - IIT-R MMFeed Dataset.pdf`][2]. The dataset is compiled by Puneet Kumar, Omkar Ingle, Daksh Goyal and Gaurav Bhatt at Machine Intelligence Lab, IIT Roorkee under the supervision of Prof. Balasubramanian Raman. It consists of 9,479 samples containing news text, images, user comments, and the number of likes for each comment.

[1]: https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=97
[2]:https://github.com/MIntelligence-Group/MMFeed/blob/main/Access%20Form%20-%20IIT-R%20MMFeed%20Dataset.pdf
