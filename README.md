Multimodal Affective Feedback Synthesis for Text and Image Data
================================================

Implementation for the paper (submitted to ICASSP 2022). <br>
**[Multimodal Affective Feedback Synthesis for Text and Image Data][1]**<br>
[Puneet Kumar](https://puneet-kr.github.io/), [Gaurav Bhatt](http://deeplearn-ai.com/), Omkar Ingle, Daksh Goyal and [Balasubramanian Raman](http://faculty.iitr.ac.in/~balarfma/)  

### Note: The code files are currently private. They will be shared soon after the paper is accepted for publication.

Dataset Access
--------------
[ToDo] Access to the ‘IIT Roorkee Multimodal Feedback (IIT-R MMFeed) dataset’ can be obtained by through [`Access Form - IIT-R MMFeed Dataset.pdf`][2]. The dataset is compiled by Puneet Kumar, Omkar Ingle, Daksh Goyal and Gaurav Bhatt at Machine Intelligence Lab, IIT Roorkee under the supervision of Prof. Balasubramanian Raman. It consists of 9,479 samples containing news text, images, user comments, and the number of likes for each comment.

[1]: https://2022.ieeeicassp.org/
[2]:https://github.com/MIntelligence-Group/MMFeed/blob/main/Access%20Form%20-%20IIT-R%MMFeed%20Dataset.pdf
